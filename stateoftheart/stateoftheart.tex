\normallinespacing
\chapter{State of the art}
\label{chap:sota}
The state of the art of this work can be divided in three parts: first, in section~\ref{sec:muexpmod}, we review the works related to music expressive performances modelling. Secondly, in section~\ref{sec:polymuexpmod} we explain the related works focused on polyphonic music expressive performances modelling. Finally, in section~\ref{sec:autohexaguit} we provide examples of works that try to automatically transcribe guitar, focusing on those treating guitar as an hexaphonic instrument transcribing each string separately.

\section{Music expression modelling}
\label{sec:muexpmod}
\subsection{Polyphonic music expression modelling}
\label{sec:polymuexpmod}
\section{Automatic hexaphonic guitar transcription}
\label{sec:autohexaguit}


Performance actions (PAs) can be defined as musical resources used by musicians to add expression when performing a musical piece, which consist of little nuances (variations in timing, pitch, and energy) that are not indicated in a score. In the same context, ornamentation can be considered as an expressive musical resource used to embellish and add expression to a melody. 

In the past, music expression has been mostly studied in the context of classical music and most research focus on studying timing deviations (onset nuances), dynamics (energy) and vibrato (pitch nuances). Some studies try to obtain rules to represent that performance actions by hand from music experts. There are several expert-based systems studying this field from different perspectives. The KTH group developed a set of several rules~(\cite{Friberg2009}) for predicting tempo, energy and pitch variations included in a system called \textit{Director Musices}. Parts of the rule system were implemented in other programs (see for instance~\cite{Sundberg2003} that tries to use rules to predict \textit{Inter Onset intervals} or~\cite{Bresin2000} who try to generate macro rules for predicting PAs).

On the other hand, machine-learning-based systems try to obtain the set of rules (expressive models) directly from the music performance by trying to directly measure the PAs applied by the performer. This PAs are computed by measuring deviations of the expressive performance (done by a professional performer) with respect to a neutral or robotic data (such as strict MIDI representations of the score). For an overview of theses methods see~\cite{Goebl2005}, from where we can see that most of the proposed expressive music systems are in classical music, and most of these systems are based on piano performances. In order to obtain these expressive performance models, several types of machine learning algorithms have been used, \cite{Bresin1998} tries to model piano performances using neural networks (\cite{Camurri2000} applied NN to flute performances), \cite{Widmer2003a} (\cite{Widmer2003}) used rule-based learning and meta-learning algorithms in order to cluster piano performances, \cite{Grindlay2006} utilizes Hidden Markov Models and \cite{Miranda2010} who use a generative performance system based on genetic algorithms to construct those rules. 

Contrary to classical music scores, performance annotations (e.g. ornaments, dynamics and articulations ) are seldom indicated in popular music scores, and it is up to the performer to include them based on his/her musical background. Therefore, in popular music it may not always be possible to characterize ornaments with the archetypal classical music conventions (e.g. trills and \textit{appoggiaturas}). 

Several approaches have been proposed to generate expressive performances not in piano-classical music. \cite{Arcos1998} proposed a system that generates jazz solo saxophone expressive performances, based on case-based reasoning. \cite{Grachten2006} also applies case-based reasoning to generate models for ornamentation and tempo variations for jazz saxophone music. \cite{Ramirez2006} compare different machine learning algorithms and techniques to obtain jazz saxophone performance models capable of synthesizing expressive performances and explaining those transformations. \cite{Puiggros2006} try to generate automatic characterization of ornamentation from bassoon recordings in order to generate expressive synthesis.

Previous work on guitar expressive performance modelling has mainly been done by \cite{Giraldo2016} who use machine learning techniques to model ornamentation and PAs in monophonic jazz guitar performances according to the characteristics of the notes' context.
\cite{bantula2016} models expressive performance for a jazz ensemble of guitar and piano. The interesting part of this work is the polyphonic treatment done to the piano, extracting features for chords played such as \textit{density, weight} or \textit{range}. \cite{KirkeAlexisMiranda2013} models polyphonic piano recordings and generative experiments that show that multiple polyphonic expressive actions can be found in human expressive performances. 

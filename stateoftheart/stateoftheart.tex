\normallinespacing
\chapter{State of the art}
\label{chap:sota}
In this section we will review the state of the art in music expression, giving an
overview of the past and present research in the field. Specifically focusing in polyphonic music expression modelling where machine learning has been used to predict some kind of performance actions.

The state of the art of this work can be divided in two parts: first, in section~\ref{sec:muexpmod}, we review the works related to music expressive performances modelling. Finally, in section~\ref{sec:autohexaguit} we provide examples of works that try to automatically transcribe guitar, focusing on those treating guitar as an hexaphonic instrument transcribing each string separately. 

\section{Music expression modelling}
\label{sec:muexpmod}

Performance Actions (PAs) can be defined as musical resources used by musicians to add expression when performing a musical piece, which consist of little nuances (variations in timing, pitch, and energy) that are not indicated in a score. In the same context, ornamentation can be considered as an expressive musical resource used to embellish and add expression to a melody. This PAs are what make music expressive and differentiate it from a robotic performance, this little nuances, done mostly unconsciously, are part of our human nature and it's what make us feel and enjoy a musical performance as something unique. This uniqueness of a performance based on the variation in timing, dynamics, timbre and pitch was first proposed by Juslin~\cite{Juslin2001}. Ramirez and Hazan~\cite{Ramirez2006} add the punctuation that those little variations should be clearly distinguishable for listeners.

In the past, music expression has been mostly studied in the context of classical music and most research focus on studying timing deviations (onset nuances), dynamics (energy) and vibrato (pitch nuances). Some studies try to obtain rules to represent that performance actions by hand from music experts. There are several expert-based systems studying this field from different perspectives. The KTH group developed a set of several rules~(\cite{Friberg2009}) for predicting tempo, energy and pitch variations included in a system called \textit{Director Musices}. Parts of the rule system were implemented in other programs (see for instance the work by Sundberg~\cite{Sundberg2003} that tries to use rules to predict \textit{Inter Onset intervals} or Bresin~\cite{Bresin2000} who try to generate macro rules for predicting PAs).

\input{Tables/sota_experts}

On the other hand, machine-learning-based systems try to obtain the set of rules (expressive models) directly from the music performance by trying to directly measure the PAs applied by the performer. This PAs are computed by measuring deviations of the expressive performance (done by a professional performer) with respect to a neutral or robotic data (such as strict MIDI representations of the score). For an overview of theses methods see the review by Goebl~\cite{Goebl2005}, from where we can see that most of the proposed expressive music systems are in classical music, and most of these systems are based on piano performances. Those kind of machine-learning systems started arising since simple synthesiser keyboards or digital pianos were used to capture expressive performances. Those devices allowed accurate timing and loudness data to be sent via MIDI (Musical Instrument Digital Interface) to a computer.

In order to obtain these expressive performance models, several types of machine learning algorithms have been used, Bresin~\cite{Bresin1998} tries to model piano performances using Artificial Neural Networks (ANN) by trying to learn automatically the Director musices rules stated by KTH group. Camurri~\cite{Camurri2000} also applied ANN in order to obtain music expression for flute performances. He also developed a 2D representation of the expression space using non-linear projections in order to be able to choose between different emotions and their middle points.

Widmer~\cite{Widmer2003a} (also in~\cite{Widmer2003}) used rule-based learning and meta-learning algorithms in order to cluster piano performances. He developed a new rule discovery algorithm named PLCG (Partition+Learn+Cluster+Generalize) that can find simple, robust partial rules models (sets of classification rules) in complex data where it is difficult or impossible to find models that completely account for all the data. PLCG is an ensemble learning method that learns multiple models via some standard rule learning algorithm, and then combines these into one final rule set via clustering, generalization, and heuristic rule selection. He also uses this algorithm and discovered rules to predict multi-level timing and dynamics~\cite{Widmer2003}.

Grindlay~\cite{Grindlay2006} utilizes Hidden Markov Models in order to extract Performance Actions from performances from both students pianists and professional pianists in order to model different performances. He uses HMM in order to predict time variations from a non-expressive score. In his work Miranda~\cite{Miranda2010} uses a generative performance system based on genetic algorithms in order to predict those time variations. 

Contrary to classical music scores, performance annotations (e.g. ornaments, dynamics and articulations ) are seldom indicated in popular music scores, and it is up to the performer to include them based on his/her musical background. Therefore, in popular music it may not always be possible to characterize ornaments with the archetypal classical music conventions (e.g. trills and \textit{appoggiaturas}). 

Several approaches have been proposed to generate expressive performances not in piano-classical music. Arcos~\cite{Arcos1998} proposed a system that generates jazz solo saxophone expressive performances, based on case-based reasoning. In his work, several recordings of a tenor sax playing different Jazz ballads were made. These recordings were analysed to extract information related to several expressive parameters. This set of parameters and the scores constitute the set of cases of a case-based system. From this set of cases, the system infers a set of possible expressive transformations for a given new phrase applying similarity criteria, based on background musical knowledge, between this new phrase and the set of cases.

Gratchen~\cite{Grachten2006} also applies case-based reasoning to generate models for ornamentation and tempo variations for jazz saxophone music. He's system automatically performs melodic and expressive analysis, and when a new musical performance must be tempo-transformed, it uses the most similar example tempo-transformation to infer the changes of expressiveness that are necessary to make the result sound natural.

Ramirez~\cite{Ramirez2006} generates a tool in order to both generating and explaining expressive music performances of monophonic Jazz melodies for saxophone. The tool consists of three components a melodic transcription component which extracts a set of acoustic features from monophonic recordings, a machine learning component which induce both an expressive transformation model and a set of expressive performance rules from the extracted acoustic features, and a melody synthesis component which generates expressive monophonic output (MIDI or audio) from inexpressive melody descriptions using the induced expressive transformation model.

Puiggros~\cite{Puiggros2006} try to generate automatic characterization of ornamentation from bassoon recordings in order to generate expressive synthesis. His work addresses the characterization of expressive bassoon ornaments by analysing audio recordings played by a professional bassoonist. This characterization is then used to generate expressive ornaments from symbolic representations by means of Machine Learning

Previous work on guitar expressive performance modelling has mainly been done by Sergio Giraldo~\cite{Giraldo2016} who use machine learning techniques to model ornamentation and PAs in monophonic jazz guitar performances according to the characteristics of the notes' context. Features extracted from scores and their corresponding audio recordings performed by a professional guitarist are used to train computational models for predicting melody ornamentation. Several machine learning techniques were explored to induce regression models for timing, onset, and dynamics (i.e. note duration and energy) transformations, and an ornamentation model for classifying notes as ornamented or non-ornamented.

Bantula~\cite{bantula2016} models expressive performance for a jazz ensemble of guitar and piano. The interesting part of this work is the polyphonic treatment done to the piano, extracting features for chords played such as \textit{density, weight} or \textit{range}. Kirke et al~\cite{KirkeAlexisMiranda2013} models polyphonic piano recordings with generative experiments that show that multiple polyphonic expressive actions can be found in human expressive performances. 

In table~\ref{tab:sota_ml} we can see an overview of authors, methods and instruments where music expression modelling using machine learning was applied.

\input{Tables/sota_ml}

\section{Automatic hexaphonic guitar transcription}
\label{sec:autohexaguit}
When we think about music transcription we usually think about a music expert hearing over and over a musical piece and writing it down to a traditional score notation. Defined by Klapuri~\cite{Klapuri2004} music transcription is defined as \textit{"the process of analysing an acoustic musical signal so as to write down the musical parameters of the sounds that occur in it"}. So, the traditional main goal of music transcription is to represent music as detailed as possible, so it can be accurately reproduced afterwards. 

Nowadays, we also think about music transcription as the way to convert acoustic music signal to a machine readable format, such as MIDI, XML or piano-roll representation in order to be analysed and processed produce a notation reflecting the most relevant information about the musical events within it, as an output. 	

One of the main difficulties when facing automatic music transcription is given by the number of voices a musical signal has, or the number of sound sources that are present in it. Limiting the problem to one single source (as it is in our case) makes us confront with another problem: monophonic and polyphonic sources. The main problem of this task is now pitch detection. 

While in the monophonic case is practically considered to be a solved problem within the state of the art techniques (Klapuri 2004~\cite{Klapuri2004}). However, polyphonic case is really far from being solved specially for multi-instrumental contexts. The main problem of polyphonic transcription is multiple fundamental frequency estimation and tacking and tracking, which is a very difficult task when two or more concurrent sounds contain partials that share some frequencies. Knowing in advance the different sources eases a bit the task~\cite{argenti2011}. There are some works with good results in multiple fundamental frequency detection by Klapuri~\cite{Klapuri2006} who tries to estimate multiple fundamental frequencies calculating the salience, or strength, of a F0 candidate as a weighted sum of the amplitudes of its harmonic partials. This F0 salience spectrum is found by optimization using generated training material.

In our case, working with guitar music, transcription process is a difficult task due to the polyphonic nature of the sound it emits.
As said before, limiting the player to just produce a monophonic sound eases the process as thre exist very good and state of the art approaches such as the autocorrelation method, Yin or spectral peak picking, among others.

However, the main goal of this project is to extend a monophonic system to a polyphonic one, so polyphonic transcription is one of the main tasks of it. Reviewing the literature we find a few approaches for transcribing polyphonic guitar music. Fiss \& Kwasinksi propose a system for automatic guitar audio transcription in real time~\cite{Fiss2011}. This approach is based on the STFT (Short Time Fourier Transform) to compute the spectrogram used to extract information about the peak locations. Afterwards they try to correct the note detector by taking into account the probability of each note being being produced among the six strings of the guitar, and thus, avoiding the ambiguity of polyphonic guitar.
 
As stated above, avoiding polyphony makes the transcription task much easier. So, the idea of capturing and transcribing separately each string makes a lot of sense. Asking the musician to think about the whole song but just playing the notes on one string would probably be very difficult as well as non musical at all. 

Solving this little problem, O'Grady \& Rickard~\cite{OGrady2009} proposed a solution based on the Roland GK-3 divided pick-up~\cite{gk3} which captures separately each string in order to be processed with a guitar synthesizer and create really strange and creative sounds with it. For the transcription of that six different signals they used Non-Negative Matrix Factorization (NMF) where a matrix V is factorized into two matrices W and H, with the property that all three matrices have no negative elements. In this case, for one string $W_{string}$ contained the magnitude spectrum of all possible notes played on that string, resulting $H_{string}$ as an activation matrix indicating the position in time in which each note was played.


\cleardoublepage
\normallinespacing
\chapter{State of the art}
\label{chap:sota}
In this section we will review the state of the art in music expression, giving an
overview of the past and present research in the field. Specifically focusing in polyphonic music expression modelling where machine learning has been used to predict some kind of performance actions.

The state of the art of this work can be divided in three parts: first, in section~\ref{sec:muexpmod}, we review the works related to music expressive performances modelling. Secondly, in section~\ref{sec:polymuexpmod} we explain the related works focused on polyphonic music expressive performances modelling. Finally, in section~\ref{sec:autohexaguit} we provide examples of works that try to automatically transcribe guitar, focusing on those treating guitar as an hexaphonic instrument transcribing each string separately. 

\section{Music expression modelling}
\label{sec:muexpmod}

Performance actions (PAs) can be defined as musical resources used by musicians to add expression when performing a musical piece, which consist of little nuances (variations in timing, pitch, and energy) that are not indicated in a score. In the same context, ornamentation can be considered as an expressive musical resource used to embellish and add expression to a melody. This PAs are what make music expressive and differentiate it from a robotic performance, this little nuances, done mostly unconsciously, are part of our human nature and it's what make us feel and enjoy a musical performance as something unique. This uniqueness of a performance based on the variation in timing, dynamics, timbre and pitch was first proposed by Juslin~\cite{Juslin2001}. Ramirez and Hazan~\cite{Ramirez2006} add the punctuation that those little variations should be clearly distinguishable for listeners.

In the past, music expression has been mostly studied in the context of classical music and most research focus on studying timing deviations (onset nuances), dynamics (energy) and vibrato (pitch nuances). Some studies try to obtain rules to represent that performance actions by hand from music experts. There are several expert-based systems studying this field from different perspectives. The KTH group developed a set of several rules~(\cite{Friberg2009}) for predicting tempo, energy and pitch variations included in a system called \textit{Director Musices}. Parts of the rule system were implemented in other programs (see for instance the work by Sundberg~\cite{Sundberg2003} that tries to use rules to predict \textit{Inter Onset intervals} or Bresin~\cite{Bresin2000} who try to generate macro rules for predicting PAs).

On the other hand, machine-learning-based systems try to obtain the set of rules (expressive models) directly from the music performance by trying to directly measure the PAs applied by the performer. This PAs are computed by measuring deviations of the expressive performance (done by a professional performer) with respect to a neutral or robotic data (such as strict MIDI representations of the score). For an overview of theses methods see the review of different of these methods by Goebl~\cite{Goebl2005}, from where we can see that most of the proposed expressive music systems are in classical music, and most of these systems are based on piano performances. In order to obtain these expressive performance models, several types of machine learning algorithms have been used, Bresin\cite{Bresin1998} tries to model piano performances using neural networks (Camurri\cite{Camurri2000} applied NN to flute performances), Widmer~\cite{Widmer2003a} (also in~\cite{Widmer2003}) used rule-based learning and meta-learning algorithms in order to cluster piano performances, Grindlay~\cite{Grindlay2006} utilizes Hidden Markov Models and Miranda~\cite{Miranda2010} who use a generative performance system based on genetic algorithms to construct those rules. 

Contrary to classical music scores, performance annotations (e.g. ornaments, dynamics and articulations ) are seldom indicated in popular music scores, and it is up to the performer to include them based on his/her musical background. Therefore, in popular music it may not always be possible to characterize ornaments with the archetypal classical music conventions (e.g. trills and \textit{appoggiaturas}). 

Several approaches have been proposed to generate expressive performances not in piano-classical music. Arcos~\cite{Arcos1998} proposed a system that generates jazz solo saxophone expressive performances, based on case-based reasoning. Gratchen~\cite{Grachten2006} also applies case-based reasoning to generate models for ornamentation and tempo variations for jazz saxophone music. Ramirez~\cite{Ramirez2006} compare different machine learning algorithms and techniques to obtain jazz saxophone performance models capable of synthesizing expressive performances and explaining those transformations. Puiggros~\cite{Puiggros2006} try to generate automatic characterization of ornamentation from bassoon recordings in order to generate expressive synthesis.

Previous work on guitar expressive performance modelling has mainly been done by Sergio Giraldo~\cite{Giraldo2016} who use machine learning techniques to model ornamentation and PAs in monophonic jazz guitar performances according to the characteristics of the notes' context.
Bantula~\cite{bantula2016} models expressive performance for a jazz ensemble of guitar and piano. The interesting part of this work is the polyphonic treatment done to the piano, extracting features for chords played such as \textit{density, weight} or \textit{range}. Kirke et al~\cite{KirkeAlexisMiranda2013} models polyphonic piano recordings and generative experiments that show that multiple polyphonic expressive actions can be found in human expressive performances. 

\subsection{Polyphonic music expression modelling}
\label{sec:polymuexpmod}

\section{Automatic hexaphonic guitar transcription}
\label{sec:autohexaguit}
When we think about music transcription we usually think about a music expert hearing over and over a musical piece and writing it down to a traditional score notation. Defined by Klapuri~\cite{Klapuri2004} music transcription is defined as \textit{"the process of analysing an acoustic musical signal so as to write down the musical parameters of the sounds that occur in it"}. So, the traditional main goal of music transcription is to represent music as detailed as possible, so it can be accurately reproduced afterwards. 

Nowadays, we also think about music transcription as the way to convert acoustic music signal to a machine readable format, such as MIDI, XML or piano-roll representation in order to be analysed and processed produce a notation reflecting the most relevant information about the musical events within it, as an output. 	

One of the main difficulties when facing automatic music transcription is given by the number of voices a musical signal has, or the number of sound sources that are present in it. Limiting the problem to one single source (as it is in our case) makes us confront with another problem: monophonic and polyphonic sources. The main problem of this task now is pitch detection. 

While in the monophonic case is practically considered to be a solved problem within the state of the art techniques (Klapuri 2004~\cite{Klapuri2004}). However, polyphonic case is really far from being solved specially for multi-instrumental contexts. The main problems of this case are present when two or more concurrent sounds contain partials that share some frequencies. Knowing in advance the different sources eases a bit the task~\cite{argenti2011}. 





























\cleardoublepage
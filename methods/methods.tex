\chapter{Methodology}
\label{chap:methods}
In figure~\ref{fig:diagram} we present a block diagram of the whole system from where we can see that four separate stages of this thesis can be defined: data acquisition (guitar recording), transcription, feature extraction and models computation. Expressive hexaphonic guitar recordings will be done using the Roland GK-3 divided pick-up, which is able to separate sound from each string~\cite{Angulo2016}. The main output of this first stage will be a new dataset consisting of hexaphonic recordings recorded by a guitar player with different performance actions of the performance. 

\begin{figure}[ht!]
\caption{Block diagram of the whole system.}
\label{fig:diagram}
\centering
\includegraphics[width=\textwidth]{Figures/Diagram.pdf}
\end{figure}

After this step, transcription of each individual string will be computed applying non-negative matrix factorization~\cite{OGrady2009}. After doing a score alignment with the original score and the transcription of the expressive guitar performance, feature extraction needs to be done.

Feature extraction will be performed following an approach in which each note is characterized by its \textit{nominal}, \textit{neighbouring}, and \textit{contextual} properties.  Here is where the most of the research in this thesis will take place:  checking for literature in expressive piano modelling, combining it with previously mentioned features of monophonic expressive guitar modelling,... Afterwards, several machine learning and feature selection algorithms will be applied to predict those performance actions (timing, pitch, energy,...) and ornaments introduced by the musician when performing a musical piece.

\section{Data acquisition}
In order to obtain hexaphonic recordings and get each string nicely separated we used the Roland GK-3 divided pick-up that is easily attached to any steel-stringed electric guitar and acts as a sound transducer device. It is able to separate very good the sound from each string and delivers accurate performance data.

However, the output of this pick-up consists of a 13 pin DIN cable that allows to connect the guitar to guitar synthesizers such as Roland's popular GR-55 and at the same time to fed electrically the pick-up. So, in order to be able to record each string separately we need to adapt the pick-up output so the sound of each string can be inputted to the computer through an independent input channel of an audio interface.

To do this, a Breakout Box circuit was built by I.Angulo~\cite{Angulo2016} for his master's thesis last year so we reused it. The final box has an input for the 13 pin DIN cable and 6 separate Jack connector cables are outputted, one for each string. Also, two batteries are needed inside the box in order to fed the pick-up. 

In this study, the final data set consists of 3 audio recordings (one recording of \textit{Darn that dream} by Jimmy Van Heusen and Eddie De.Lange and two recordings of \textit{Suite en La} by Manuel M. Ponce) resulting in a total of 1414 notes recorded by an amateur guitarist and their corresponding music scores saved as xml file using Muse Score 2. Each of these notes was characterized by a set of descriptors as explained in the following sections.  

\section{Hexaphonic guitar transcription}
The aim of this section is to obtain machine readable (and understandable) representation from the audio recordings in order to be able to compute descriptors and performance actions. So, the audio signal from each string from the guitar was automatically transcribed into a MIDI format, in order to obtain a note representation based on pitch, onset, duration and offset for each note. This step was based on the previous work of Bantula, Giraldo and Ram√≠rez~\cite{bantula2016}. However, the algorithm has been modified a bit in order to correctly transcribe the hexaphonic audio (that contain lickage from the other strings) instead of simple monophonic ones.

For doing this, we first need a fundamental frequency detection in order to obtain the pitch of each string. The original algorithm used the YIN algorithm~\cite{Cheveigne2002}, however, in order to improve the system we changed it to Melodia which offers more robustness against the lickage from other strings and helps to detect the main pitch present in the signal. In order to provide a better F0 detection, we also tuned the parameters of this algorithm. The minimum and maximum frequency (the range) of the detector was set differently to each string according to the frequencies that a guitar can produce. As can be seen in table~\ref{tab:guitar_strings} we considered from each string a range of one octave because in this particular recordings and scores the guitarist never passes the twelfth fret.

\input{Tables/guitar_strings}
The power envelope of the signal is used to apply an adaptive noise gate in order to filter out none pitched sounds. That power envelope was also used in order to compute MIDI velocity (energy was linearly mapped to a value from 0 to 127). Finally, the filtered frequency pitch profile obtained is rounded to a MIDI note number.

In figure~\ref{fig:transcription} we can see the four stages of the process: we start from the audio (a), we obtain a pitch profile using Melodia (b) and using an adaptative threshold over the audio wave envelope (c) we obtain a filtered pitch profile.

Following the algorithm proposed by Bantula, a rule based filter is applied in order to remove short notes and gaps merging them with corresponding neighbour notes based on a cognitive perspective of the perception of time.

\begin{figure}[ht!]
\caption{One string automatic transcription}
\label{fig:transcription}
\centering
\includegraphics[width=\textwidth]{Figures/transcription.pdf}
\end{figure}

Following previous step, onsets and offsets are detected from differentiating the cleaned pitch profile. This means for a pitch remove next one, so the changes in pitch become positive or negative peaks. Positive ones (above a threshold) are labelled as onsets and negative ones (below a threshold) are labelled as offsets. Duration is computed by subtracting the offset to the onset.

After all this process, a few manual corrections were performed by changing pitch, eliminating notes or time stretching the performance in order to have a better alignment with the score. Afterwards, all six transcriptions (one from each string) were merged in order to obtain a single MIDI file from the performance. MIDI channel was used in order to label the notes according to the string where were played.

\section{Feature extraction}
In this section we describe how we analysed the music scores in order to extract multiple descriptors for each note. Afterwards, performance to score alignment was computed and performance actions such as onset deviation or energy ratio were extracted.

\subsection{Note Descriptors}
Feature extraction from the music scores was performed following an approach similar to that of Giraldo~\cite{Giraldo2016} but extended and computationally adapted to polyphonic scores, in which each note is characterized by its \textit{nominal}, \textit{neighbouring} and \textit{contextual} properties, taking into account both horizontal (time or melodic) and vertical (simultaneous or harmonic) axis. The complete list of the descriptors extracted from the music scores can be found in Table~\ref{tab:note_descriptors}.

\input{Tables/note_descriptors}

\begin{itemize}
\item \textbf{Nominal}: This descriptors refer to the intrinsic or intra-note properties of score notes. So to say, this descriptors are needed in term to define completely a note. \textit{Duration} (computed from offset and onsets) and \textit{onset} are given both in seconds and beats, as the descriptor in seconds depends on the tempo of the piece. \textit{Onset in bar} refers to the position of the onset related to the beats per bar measure. If its 1 the first beat on the bar, and so on. The \textit{pitch} is directly the MIDI note number, and the \textit{chroma} is the pitch modulus 12, so to say, the pitch class of the note, the pitch without taking into account octave changes. Energy descriptor gives us how loud is the note played (the loudness), and its directly taken from MIDI velocity (how fast the note is played). Finally, \textit{string} number is also extracted for each note directly from the MIDI channel.

\item \textbf{Neighbouring}: This neighbouring or inter-note descriptors refer to the relations of the note with its neighbouring or simultaneous notes. Each note is characterized by the \textit{Previous duration} and \textit{Next duration} given both in seconds and beats. Also \textsc{Previous interval} and \textit{Next interval} is given in semitones. \textit{Inter-onset distance} refers to the onset difference between current and previous or next note. \textit{Simultaneous notes} counts the number of simultaneous notes to the current note within a given thresholds.

Following the work of Bantula~\cite{bantula2016} on piano polyphonic music, notes have been also labelled as \textit{chord notes} or \textit{pedal notes} depending on the simultaneity of other notes, differentiating between notes that have been played at the same time as a chord and notes that are played as a basis in order to have a melody in top of them.

In this neighbouring category of descriptors, categorization based on the implication-realization (I-R) model of \textit{Narmour} has been computed.  This model parses melodies and obtains for each note a label depending on the previous and next notes. This computation has also been adapted in order to take into account polyphonic melodies. 

\item \textbf{Contextual}: This descriptors refer to the context, background or properties of the song in which the note appears on. Some of this descriptors, such as \textsc{measure}, \textit{tempo}, \textit{key} or \textit{mode}, are the same for the whole song but may be useful if we merge different datasets into one. \textit{Chord root} and \textit{chord type} refer to the actual chord of that note (labelled manually per bar or per measure in the score).

Others of this descriptors are just computations of the note respecting those first. \textit{Note to key} and \textit{note to chord} refer to the distance in semitones of the actual note both to the general key root of the piece or to the actual chord root. \textit{Is chord note} gives us a boolean label depending if the note is part of those notes pre-defined by the chord root and type as can bee seen in table~\ref{tab:chord_extensions}. \textit{Metrical strength} categorize notes occurring at strong or weak beats within a bar. And \textit{phrase} descriptor labels notes depending on the melodic segmentation approach by Cambouropulos~\cite{Cambouropoulos1997a} into initial, middle and final notes.

\end{itemize}

\input{Tables/chord_extensions}

\subsection{Performance to score alignment}

In this stage, and in order to compute performance actions in the next step, we need to know which notes on the performance correspond to which notes in the score, or mostly known as performance to score alignment.

This is done automatically and in this case we will use Dynamic Time Warping (DTW) techniques in order to match performance notes to score. Those notes are alignment depending on a cost function based on onset, pitch and duration deviations. All these deviations can be weighted with a parameter in order to penalize more errors in pitch than in onset i.e. firstly, and previously to applying DTW, performances have been manually time-stretched in order to obtain a better automatic alignment. Then we compute a cost or similarity matrix of the notes on the performance against score notes. As we can see in figure~\ref{fig:suite_cost_matrix} after the cost matrix computation, an optimal path is retrieved in order to found the alignment with less global cost.

\begin{figure}[ht!]
\caption{\textit{Suite en La} similarity matrix with optimal path,}
\label{fig:suite_cost_matrix}
\centering
\includegraphics[width=0.8\textwidth]{Figures/Suite_en_la_cost.pdf}
\end{figure}

Some restrictions have been done to this optimal path computation in order to apply some rules. Horizontal paths are forbidden in order to ensure that each performance note has only one score note as reference, and be able to compute embellishments. However, we need to allow vertical paths so one score note can be played as several performance notes. In figure~\ref{fig:darn_auto_alignment} we can see an example of an automatic performance to score alignment. In this plot, score has been shifted two octaves up in order to have a better visualization.

\begin{figure}[ht!]
\caption{\textit{Darn that dream} performance to score alignment}
\label{fig:darn_auto_alignment}
\centering
\includegraphics[width=\textwidth]{Figures/Darn_that_dream_auto.pdf}
\end{figure}



\subsection{Performance actions}

\section{Machine Learning modelling}
\cleardoublepage


\normallinespacing

\chapter{Introduction}

\label{chap:introduction}
Music is a very important part of the life of most people. Depending on our mood or moment in the day or lives, music can be understood in very different ways. In some moments we understand music as a simple distraction, a soundtrack to our daily tasks without paying much attention to it. In other moments, when we consciously listen to music, we can be very touched and excited by it. This engaging part of music is largely due to the human component added to the performance. Instead of reading a score, musicians play the music on their own way, by changing (unconsciously or consciously if the performer wants to achieve a specific goal) a lot of "parameters" of it such as intensity, velocity, volume or articulation of each note. Moreover, people can clearly distinguish the manipulation of sound properties done by different performers and create preferences based on these differences. 

The study of music expressive performance, from a computational point of view, consists of characterizing the deviations that a musician, when performing a musical piece, introduces in the performance. In this work we are going to focus on modelling guitar scores and performances. In Section~\ref{sec:structure} we briefly explain how this thesis is organised. 

Computational modelling of expressive music performance has been widely studied in the past. While previous work in this area has been mainly focused on classical piano music, there has been very little studies on guitar music, and such work has focused on monophonic guitar playing. One of the main challenges of focusing this study to guitar is the polyphonic nature of the guitar. The complexity of polyphonic sound transcription is well known, so to solve this issue, the use of a hexaphonic guitar is chosen, in which each of the strings is processed as an independent monophonic sound source, simplifying the transcription of the sounds. 

In this thesis, we present a machine learning approach to automatically generate expressive performances from non expressive music scores for polyphonic guitar. We treated guitar as an hexaphonic instrument, by transcribing each string separately we were able to obtain a polyphonic transcription of performed musical pieces. Features were extracted from the scores and Performance Actions were computed from the deviations of the score and the performance. Machine learning techniques were used to train computational models in order to predict the aforementioned Performance Actions. Qualitative and quantitative evaluations of the models and the predicted pieces were performed. 


\section{Structure of the report}
\label{sec:structure}
In this chapter we discuss the motivation of this master's thesis, the main objectives and we explain briefly the structure of this report.
The rest of this thesis is organised as follows: in Chapter~\ref{chap:sota}, we present some related work on expressive music performance modelling specially focused on polyphonic music. In Chapter~\ref{chap:materials}, the tools and resources (hardware, software and data) used in this work are described. In Chapter~\ref{chap:methods}, we present the proposed methodology. In Chapter~\ref{chap:results}, the evaluation measures and results are presented. We conclude with a brief conclusion and provide suggestions for future improvements in Chapter~\ref{chap:conclusions}. 

In order to complement this thesis we present 3 Appendices: Appendix~\ref{app:code} providing links to all On-line resources from this thesis, Appendix~\ref{app:dataset} documenting the dataset used for this work, and Appendix~\ref{app:survey} gathering all responses to the On-line Survey.

\section{Motivation}
\label{sec:motivation}
While most studies about guitar modelling are focused on monophonic performances, the aim of this master thesis is to investigate the modelling of expressiveness in polyphonic (hexaphonic) guitar music. We will base our approach on previous studies by Giraldo~\cite{Giraldo2016} who computed expressive performance models for monophonic jazz guitar. Treating guitar as a monophonic instrument limits hardly the polyphonic nature of the instrument, but avoids the problems related to polyphonic music transcription. The main objective of this thesis will be to define a set of features extending previous work on monophonic guitar performances to polyphonic performances. Those features aim to represent the different nuances in time, duration or volume that the guitarist introduces when performing a musical piece, appearing both in the temporal or \textit{horizontal} axis (as a monophonic melody), but also should represent the \textit{vertical} axis representing the simultaneity between notes. The features should represent the variations in time and energy that the performer introduces, and those will depend on the context of the note if it is part of the melody or the harmonic accompaniment.

I personally believe that this concept of music expression plays a major role in how we appreciate musical experiences. A musical piece does not sound the same (or we do not feel it the same way) played by two different players. Even the same piece does not sound the same when played twice by the same player. 

Having a knowledge about the exact Performance Actions that a guitar player performs when reading and performing a musical piece could help us in many directions. By just replicating these nuances we would theoretically be able to sound like a concrete guitar player. Understanding little nuances that expert players perform could also help less-trained musicians to improve their playing. These models could also be implemented in music annotation software, in order to generate expressive performances from user-composed scores. That way, the plain score to midi conversion could be substituted by an expressive playback, much closer to the performance of an expert guitarist, and thus improve the overall user experience.

\section{Thesis statement}
In this section we present the main hypothesis of this master's thesis:

\textit{It is possible to computationally capture and model the expressive nuances that a musician introduces when performing a musical piece, taking the polyphonic guitar as a study case.}


\section{Objectives}
The aim of this work is to study and predict computationally predict the little nuances or \textit{Performance Actions} that musicians do when performing a musical score, focusing on time (\textit{Onset Deviation}) and amplitude (\textit{Energy ratio}) deviations, taking the polyphonic (hexaphonic) guitar as a study case. This study will consider, as explained in section~\ref{sec:motivation}, both horizontal or melodic axis and vertical or harmonic axis.

The specific objectives are as follows:

\begin{itemize}[noitemsep]
\item To create a database of hexaphonic recordings played by a guitarist and their corresponding scores.
\item To automatically transcribe the audio of the hexaphonic recordings into a machine-readable format (MIDI).
\item To adapt existing code libraries to extract descriptors from the score which allow us to characterise the notes vertically and horizontally.
\item To create code libraries which allow us to align and compare the transcribed hexaphonic recordings to the score in order to extract performance actions.
\item To provide some examples of polyphonic performance to score alignments.
\item To generate different models that try to predict performance actions (onset deviation and energy ration) by using Machine Learning techniques.
\item To analyse which descriptors influence more the accuracy of these models, so to say, which descriptors represent more the behaviour of the musician.
\item To obtain not only quantitative machine learning results but also qualitative results by surveying different users.
\end{itemize}

\cleardoublepage

